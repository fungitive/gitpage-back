<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubeadm安装Kubernetes1.11集群]]></title>
    <url>%2F2018-09-01-kubeadm%E5%AE%89%E8%A3%85Kubernetes1-11%E9%9B%86%E7%BE%A4-1%2F</url>
    <content type="text"><![CDATA[集群环境 主机名称 IP 备注 操作系统 master 192.168.0.8 docker、kubectl、kubelet、kubeadm、flannel centos7.3 node01 192.168.0.9 docker、kubectl、kubelet、kubeadm centos7.3 node02 192.168.0.10 docker、kubectl、kubelet、kubeadm centos7.3 软件版本 kubernetes：1.11.2 docker-ce：18.06.1-ce flennal：master 一、环境初始化1、分别在各节点设置主机名称 hostnamectl set-hostname master hostnamectl set-hostname node01 hostnamectl set-hostname node02 2、配置主机映射(各节点都需要) cat &lt;&lt;EOF &gt; /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.0.8 master 192.168.0.9 node01 192.168.0.10 node02 EOF 3、关闭防火墙 systemctl stop firewalld &amp;&amp; systemctl disable firewalld 4、关闭Selinux setenforce 0 #临时禁用selinux sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux #永久关闭 修改/etc/sysconfig/selinux文件设置 sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config 5、关闭Swap，否则kubelet会出错！ swapoff -a #临时关闭swap sed -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstab #永久关闭 注释/etc/fstab文件里swap相关的行 6、配置路由 cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF 使其立刻生效 sysctl --system 或执行 sysctl -p /etc/sysctl.d/k8s.conf生效 7、安装依赖包配置ntp yum install -y epel-release yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl systemctl enable ntpdate.service echo &apos;*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com &gt;/dev/null 2&gt;&amp;1&apos; &gt; /tmp/crontab2.tmp crontab /tmp/crontab2.tmp systemctl start ntpdate.service 8、添加kubernetes的yum源 cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 9、设置内核（可不设置） echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.conf echo &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.conf echo &quot;* soft nproc 65536&quot; &gt;&gt; /etc/security/limits.conf echo &quot;* hard nproc 65536&quot; &gt;&gt; /etc/security/limits.conf echo &quot;* soft memlock unlimited&quot; &gt;&gt; /etc/security/limits.conf echo &quot;* hard memlock unlimited&quot; &gt;&gt; /etc/security/limits.conf 自己写的一个初始化脚本config.sh ，可以提高初始化效率。 二、安装与配置docker1、安装docker参照《Centos7安装Docker最新版》2、配置docker镜像下载代理 vi /usr/lib/systemd/system/docker.service的ExecStart前加入一行 Environment=&quot;HTTPS_PROXY=http://ik8s.io:10080&quot; Environment=&quot;NO_PROXY=127.0.0.0/8,172.20.0.0/16&quot; 3、重启docker systemctl daemon-reload &amp;&amp; systemctl restart docker 三、安装与配置kubeadm, kubelet和kubectl1、安装kubeadm, kubelet和kubectl yum install -y kubelet kubeadm kubectl 2、配置kubeadm vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf修改如下 Environment=&quot;KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/ --cni-bin-dir=/opt/cni/bin&quot; systemctl enable kubelet &amp;&amp; systemctl start kubelet 4: 命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source &lt;(kubectl completion bash) echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc 四、使用kubeadm初始化master初始化的时候指定一下kubernetes版本，并设置一下pod-network-cidr（后面的flannel会用到）： $ kubeadm init --kubernetes-version=v1.11.2 --pod-network-cidr=10.244.0.0/16 [root@master]# kubeadm init --kubernetes-version=v1.11.2 --pod-network-cidr=10.244.0.0/16 [init] using Kubernetes version: v1.11.2 [preflight] running pre-flight checks I0825 11:41:52.394205 5611 kernel_validator.go:81] Validating kernel version I0825 11:41:52.394466 5611 kernel_validator.go:96] Validating kernel config [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [preflight] Activating the kubelet service [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.8] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated sa key and public key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [master localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.0.8 127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 49.502361 seconds [uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [kubelet] Creating a ConfigMap &quot;kubelet-config-1.11&quot; in namespace kube-system with the configuration for the kubelets in the cluster [markmaster] Marking the node master as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot; [markmaster] Marking the node master as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master&quot; as an annotation [bootstraptoken] using token: 3resfo.cam2tnjxw0tastur [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.8:6443 --token 3resfo.cam2tnjxw0tastur --discovery-token-ca-cert-hash sha256:4a4f45a3c7344ddfe02af363be293b21237caaf2b1598c31d6e662a18bb76fd9 设置config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装flannel，中间的版本号换为master即为最新版。 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml 安装完network之后，你可以通过kubectl get pods –all-namespaces来查看kube-dns是否在running来判断network是否安装成功。 五、将node加入集群1、配置kubelet从master将kubelet文件分别复制到node01、node02 scp /etc/sysconfig/kubelet node01:/etc/sysconfig/kubelet scp /etc/sysconfig/kubelet node02:/etc/sysconfig/kubelet 2、执行 kubeadm join的命令即可： kubeadm join 192.168.0.8:6443 --token 3resfo.cam2tnjxw0tastur --discovery-token-ca-cert-hash sha256:4a4f45a3c7344ddfe02af363be293b21237caaf2b1598c31d6e662a18bb76fd9 六、测试kubectl get nodes 集群部署成功 七、初始化集群报错及问题解决：问题一： [kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters. unable to fetch release information. URL: &quot;https://storage.googleapis.com/kubernetes-release/release/stable-1.7.5.txt&quot; Status: 404 Not Found 解决： 添加版本信息“--kubernetes-version=v1.7.5”，kubeadm reset，再次执行init 问题二： W1205 18:49:21.323220 106548 cni.go:189] Unable to update cni config: No networks found in /etc/cni/net.d Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 解决：修改文件内容： /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Environment=&quot;KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf- dir=/etc/cni/ --cni-bin-dir=/opt/cni/bin&quot; 问题三： k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://192.168.0.8:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&amp;resourceVersion=0: dial tcp 192.168.0.8:6443: getsockopt: connection refused k8s.io/kubernetes/pkg/kubelet/kubelet.go:400: Failed to list *v1.Service: Get https://192.168.0.8:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.8:6443: getsockopt: connection refused k8s.io/kubernetes/pkg/kubelet/kubelet.go:408: Failed to list *v1.Node: Get https://192.168.0.8:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&amp;resourceVersion=0: dial tcp 192.168.0.8:6443: getsockopt: connection refused Unable to write event: &apos;Post https://192.168.0.8:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.8:6443: getsockopt: connection refused&apos; (may retry after sleeping) Failed to get status for pod &quot;etcd-k8s-master_kube-system(5802ae0664772d031dee332b3c63498e)&quot;: Get https://192.168.0.8:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.8:6443: getsockopt: connection refused 解决：打开防火墙 systemctl start firewalld 添加火墙规则： firewall-cmd --zone=public --add-port=80/tcp --permanent firewall-cmd --zone=public --add-port=6443/tcp --permanent firewall-cmd --zone=public --add-port=2379-2380/tcp --permanent firewall-cmd --zone=public --add-port=10250-10255/tcp --permanent firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent firewall-cmd --reload firewall-cmd --zone=public --list-ports 问题四： [root@master]# kubectl get node Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;) 解决： [root@master]# mv $HOME/.kube $HOME/.kube.bak [root@mster]# mkdir -p $HOME/.kube [root@master]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master]# chown $(id -u):$(id -g) $HOME/.kube/config 八、安装kubernetes-dashboard1、下载kubernetes-dashboard.yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 2、编辑kubernetes-dashboard.yaml 添加type: Nodeport 和nodePort: 30001，将146行的serviceAccountName: kubernetes-dashboard改为serviceAccountName: kubernetes-dashboard-adminkubernetes-dashboard.yaml内容如下： # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Configuration to deploy release version of the Dashboard UI compatible with # Kubernetes 1.8. # # Example usage: kubectl create -f &lt;this_file&gt; # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role &amp; Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create &apos;kubernetes-dashboard-key-holder&apos; secret. - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;create&quot;] # Allow Dashboard to create &apos;kubernetes-dashboard-settings&apos; config map. - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] verbs: [&quot;create&quot;] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;] verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] # Allow Dashboard to get and update &apos;kubernetes-dashboard-settings&apos; config map. - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;kubernetes-dashboard-settings&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] # Allow Dashboard to get metrics from heapster. - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;heapster&quot;] verbs: [&quot;proxy&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services/proxy&quot;] resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;] verbs: [&quot;get&quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimal subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1beta2 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard-admin #不改的话有坑 # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 3、安装dashboard kubectl apply -f kubernetes-dashboard.yaml 如果不授予权限就会报错。 4、授予dashboard账户集群管理权限，新建vi kubernetes-dashboard-admin.rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system 授予权限 kubectl apply -f kubernetes-dashboard-admin.rbac.yaml 6、访问dashboard https://192.168.0.10:30001 7、获取token令牌的方式访问获取token [root@master ~]# kubectl -n kube-system get secret | grep kubernetes-dashboard-admin|awk &apos;{print &quot;secret/&quot;$1}&apos;|xargs kubectl describe -n kube-system|grep token:|awk -F : &apos;{print $2}&apos;|xargs echo eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1qYnRrZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImYzZTY2NjBhLWE4NTgtMTFlOC1iNTI2LTAwMGMyOWU2ZTA4MiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.CcgvvsCEkwKi0nhq-cnm-rDmLiBSclnK3H3cTugUpawvS2ruBl05jVpwPyh3pNc4Z_V5GPelTa7tsVJHDQ2uG1P7HYqKkcvtFnua9y5DAFMqtOf-sxiHSDjIkphXDKCxRVaGXQzv9bTC-MAT0NnJzK08w8lZlITWDuT_GQQHcczCOVknFnwVFDEzQKR0DLc9Bx2Gw-5TINidjhVHIWmUMhfEZE5F1D_kvBHRS6bgE43h0OsoPqs3BeCzxRTCbdbeDb9wIVcBxoi9QF9pE5k5dyuNOylRP2SLiHrK8nuCZSESZkRSDkC_3M2ax_2yfnBGi1cwH1A4JAgcMr7iIIBKAg 将令牌复制登录即可]]></content>
      <categories>
        <category>容器云</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python标准库]]></title>
    <url>%2F2018-08-31-2018-07-10-Python-%E6%A0%87%E5%87%86%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Python丰富而强大遍历的标准库是其最突出的优点之一. 这里当然不可能说明所有的标准库, 本文只简单列出一些标准库的使用, 还有一部分在其它博文中说明: Python时间系统 Python解析xml与JSON requests发送HTTP请求 PDB调试Python程序 Python日志模块logging Python正则表达式模块re Python虚拟环境virtualenv Python 数据库 Python Socket Python WSGI接口 使用urllib访问网络资源 操作系统环境os 建议使用 import os 风格而非 from os import *。这样可以保证随操作系统不同而有所变化的 os.open() 不会覆盖内置函数 open()。 在使用 os 这样的大型模块时内置的 dir() 和 help() 函数非常有用。命令行参数sys 通用工具脚本经常调用命令行参数。这些命令行参数以链表形式存储于 sys 模块的 argv 变量。 sys 还有 stdin，stdout 和 stderr 属性，即使在 stdout 被重定向时，后者也可以用于显示警告和错误信息。 &gt;&gt;&gt; sys.stderr.write(&apos;Warning, log file not found starting a new one\n&apos;) Warning, log file not found starting a new one 数学math数据压缩zlib 以下模块直接支持通用的数据打包和压缩格式：zlib，gzip，bz2，zipfile，以及 tarfile。 &gt;&gt;&gt; import zlib &gt;&gt;&gt; s = b&apos;witch which has which witches wrist watch&apos; &gt;&gt;&gt; len(s) 41 &gt;&gt;&gt; t = zlib.compress(s) &gt;&gt;&gt; len(t) 37 &gt;&gt;&gt; zlib.decompress(t) b&apos;witch which has which witches wrist watch&apos; &gt;&gt;&gt; zlib.crc32(s) 226805979 性能测试timeit &gt;&gt;&gt; from timeit import Timer &gt;&gt;&gt; Timer(&apos;t=a; a=b; b=t&apos;, &apos;a=1; b=2&apos;).timeit() 0.57535828626024577 &gt;&gt;&gt; Timer(&apos;a,b = b,a&apos;, &apos;a=1; b=2&apos;).timeit() 0.54962537085770791 文件通配符glob glob模块提供了一个函数用于从目录通配符搜索中生成文件列表： &gt;&gt;&gt; import glob &gt;&gt;&gt; glob.glob(&apos;*.py&apos;) [&apos;primes.py&apos;, &apos;random.py&apos;, &apos;quote.py&apos;] 高精度小数 decimal decmial模块可以提供制定精度的小数运算，使用前需要&gt;&gt;&gt;import decmial。decmial对象由构造函数decmial.Decmial()根据一个int或字符串参数建立。Python3.5中decmial.Decmial()可以根据float建立对象，但那是不精确的。 &gt;&gt;&gt;decimal.Decimal(1) Decimal(&apos;1&apos;) &gt;&gt;&gt;decimal.Decimal(&quot;1.2&quot;) Decimal(&apos;1.2&apos;) math与cmath库中的数学函数对decimal不适用，但decimal自身提供了一系列数学函数。 &gt;&gt;&gt; x = decimal.Decimal(2) &gt;&gt;&gt; x.exp() Decimal(&apos;7.389056098930650227230427461&apos;) &gt;&gt;&gt; decimal.Decimal.exp(x) Decimal(&apos;7.389056098930650227230427461&apos;) 对象序列化pickle Python的pickle模块实现了基本的数据序列和反序列化。序列化：​import picklepickle.dump(obj, file, [,protocol]) 反序列化： obj = pickle.load(file)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018-08-31-hello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker三剑客compose、machine、swarm]]></title>
    <url>%2F2018-08-31-2018-08-08-docker%2F</url>
    <content type="text"><![CDATA[docker三剑客compose、machine、swarmdocker-compose 安装compose curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 测试安装 $ docker-compose --version docker-compose version 1.21.2, build 1719ceb 常用命令 命令: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information https://docs.docker.com/compose/overview/ docker-machine简介 docker-machine是安装docker环境的一个工具，可以在一台机器上通过命令控制几台机器安装docker环境，运行docker命令，创建docker swarm集群的工具。 安装 docker-machine和compose有点类似，都是一个可运行的linux二进制文件(下面都是基于linux版本做的)，下载下来这个文件后放到/usr/local/bin里面设置文件权限就可以直接使用了，docker-machine的github地址https://github.com/docker/machine curl -L https://github.com/docker/machine/releases/download/v0.10.0/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 使用 按照docker-machine github上的介绍，它是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure。根据他的描述和github上的例子可以看出他可以直接在指定平台上创建机器。 我们这里只测试已经创建好有ip的实体机或者虚拟机。docker-machine操作各个机器实际上用ssh无密码访问的，如果是在已经配置好ip的实体机或虚拟机上用就要手动或者使用脚本设置无密码访问了。 无密码访问ssh-keygen #一直回车 ssh-copy-id root@192.168.1.28 #ip为docker-machine要操作的机器，输入密码上面结束之后，每台机器上还得安装net-tools,docker-machine会用到netstat命令来检测端口使用情况，如果机器上没有安装会报错。如果你确定那台机器上的端口没问题，即使报错也没问题，最终那台机器还是会加入到docker-machine的管理中。yum install net-tools连接机器 docker-machine create -d generic –generic-ip-address=192.168.1.28 node28 node28为给机器的别名 -d generic驱动类型 –generic-ip-address 要控制机器的ip，必须 –generic-engine-port docker-engine的远程访问端口，默认为2376 –generic-ssh-key 远程访问机器的私钥，默认使用.ssh/下面的私钥 –generic-ssh-user 远程访问机器的用户名，默认为root –generic-ssh-port 远程ssh访问的端口，默认为22 –engine-insecure-registry docker-engine的insecure-registry –engine-install-url 安装docker-engine的地址，默认为”https://get.docker.com” –engine-registry-mirror docker-engine镜像的代理地址上面的命令根据国内环境可以换为下面 docker-machine create \ -d generic \ –generic-ip-address=192.168.1.28 \ –engine-install-url=https://get.daocloud.io/docker/ \ –engine-registry-mirror=http://91c0cc1e.m.daocloud.io \ node28通过docker-machine连接了各个机器后，就可以通过docker-machine来操作各个机器了，更多命令查看 docker-machine –helphttps://docs.docker.com/machine/install-machine/https://blog.csdn.net/vchy_zhao/article/details/70238472 swarm简介swarm从docker1.9版本开始就有了，但功能不完善、性能不稳定，一直不能登入生产环境，从1.12版本内置到了docker-engine中，可以直接使用docker swarm命令来操作swarm。 swarm是docker集群的资源管理工具。简单点理解，在很多台机器上部署docker，组成一个docker集群，并把整个集群的资源抽象成资源池，使用者部署docker应用的时候，只需要将应用交给swarm，swarm会根据整个集群资源的使用情况来分配资源给部署的docker应用，可以将这个集群的资源利用率达到最大。类似的服务框架还有mesos+marathon，kubernetes。 ①最早使用的是mesos+marathon那一套，优点是基于成熟的资源调度管理框架mesos，缺点是部署起来还是很麻烦的，像服务发现、负载均衡等概念在里面也都有，但都是碎片化以插件的形式存在，整个体系感觉不是很完善、不像一个整体。 ②kubernetes从发布1.0版本以后在生产得到了很多实践，开始步入主流压过swarm和mesos+marathon，kubernetes针对docker应用集群的特点，概括出几个对象，pod、service、replication controller，pod为运行的基本单元，service则是专门来服务发现和服务代理的，replication controller 应用的副本做负载均衡。kubernetes就是一个很专业很全面完善的docker集群管理工具。 ③swarm在很多方面很像kubernetes，不知道是不是偷偷抄袭的。swarm通过命令就可以很简单的在docker集群中创建应用设置副本数量，内置服务发现代理。swarm+compose≈kubernetes。swarm由于现在内置于docker中，使用部署更简单，功能上和kubernetes很相似，轻量级。 常用命令 swarm init swarm join service create service inspect service ls service rm service scale service ps service update]]></content>
      <categories>
        <category>容器云</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python运维常用的20个库和模块]]></title>
    <url>%2F2018-08-31-2018-07-10-python-test%2F</url>
    <content type="text"><![CDATA[Python运维常用的20个库和模块 1、psutil是一个跨平台库（https://github.com/giampaolo/psutil）能够实现获取系统运行的进程和系统利用率（内存，CPU,磁盘，网络等），主要用于系统监控，分析和系统资源及进程的管理。 2、IPy（http://github.com/haypo/python-ipy）,辅助IP规划。 3、dnspython(http://dnspython.org)Python实现的一个DNS工具包。 4、difflib：difflib作为Python的标准模块，无需安装，作用是对比文本之间的差异。 5、filecmp:系统自带，可以实现文件，目录，遍历子目录的差异，对比功能。 6、smtplib：发送电子邮件模块 7、pycurl(http://pycurl.sourceforge.net)是一个用C语言写的libcurl Python实现，功能强大，支持的协议有：FTP,HTTP,HTTPS,TELNET等，可以理解为Linux下curl命令功能的Python封装。（PS：PycURL在前几天的文章里有提及过） 8、XlsxWriter:操作Excel工作表的文字，数字，公式，图表等。 9、rrdtool:用于跟踪对象的变化，生成这些变化的走走势图 10、scapy(http://www.wecdev.org/projects/scapy/)是一个强大的交互式数据包处理程序，它能够对数据包进行伪造或解包，包括发送数据包，包嗅探，应答和反馈等功能。 11、Clam Antivirus免费开放源代码防毒软件，pyClamad,可以让Python模块直接使用ClamAV病毒扫描守护进程calmd。 12、pexpect:可以理解成Linux下expect的Python封装，通过pexpect我们可以实现对ssh,ftp,passwd,telnet等命令行进行自动交互，而无需人工干涉来达到自动化的目的。 13、paramiko是基于Python实现的SSH2远程安装连接，支持认证及密钥方式。可以实现远程命令执行，文件传输，中间SSH代理等功能。相对于Pexpect,封装的层次更高，更贴近SSH协议的功能，官网地址：http://paramiko.org(依赖：Crypto,Ecdsa,Python开发包python-devel) 14、fabric是基于Python实现的SSH命令行工具，简化了SSH的应用程序部署及系统管理任务，它提供了系统基础的操作组件，可以实现本地或远程shell命令，包括命令执行，文件上传，下载及完整执行日志输出等功能。Fabric在paramiko的基础上做了更高一层的封装，操作起来更加简单。官网地址：http://www.fabfile.org(依赖setuptools,Crypto,paramiko包支持) 15、CGIHTTPRequestHandler实现对CGI的支持。 16、ansible(http://www.ansibleworks.com/)一种集成IT系统的配置管理，应用部署，执行特定任务的开源平台。基于Python实现，由Paramiko和PyYAML两个关键模块构建。Ansibl与Saltstack最大的区别是Ansible无需在被控主机上部署任何客户端，默认直接通过SSH通道进行远程命令执行或下发功能。 17、YAML:是一种用来表达数据序列的编程语言。 18、playbook：一个非常简单的配置管理和多主机部署系统。 19、saltstack(http://saltstack.com)是一个服务器基础架构集中化管理平台，一般可以理解为简化版的puppet和加强版的func。Saltstack基于Python语言实现，结合轻量级消息队列ZeroMQ,与Python每三方模块（Pyzmq,PyCrypto,Pyjinja2,python-msgpack和PyYAML等）构建。 20、func，为解决集群管理，监控问题需设计开发的系统管理基础框架。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2018-08-31-docker%2F</url>
    <content type="text"><![CDATA[docker是什么]]></content>
      <categories>
        <category>容器云</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
